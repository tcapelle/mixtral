{
    "args": [
    ],
    "run_config": {
        "axolotl_config": {
            "max_steps": 10,
            "wandb_log_model": "checkpoint",
            "save_steps": 6,
            "eval_steps": 0.9,
            "hub_model_id": "wandb/Mixtral-8x7b-Remixtral",
            "hub_strategy": "checkpoint",
            "output_dir": "/wb-mixtral/slimorca-mixstral-8x7b_v2",
            "save_strategy": "steps",
            "overwrite_output_dir": true,
            "bf16": true,
            "fp16": false,
            "fsdp": null,
            "tf32": true,
            "debug": null,
            "lora_r": 16,
            "strict": false,
            "tokens": null,
            "adapter": "qlora",
            "datasets": [
                {
                    "path": "Open-Orca/SlimOrca",
                    "type": "sharegpt",
                    "conversation": "chatml"
                }
            ],
            "deepspeed": null,
            "optimizer": "paged_adamw_8bit",
            "adam_beta2": 0.95,
            "base_model": "mistralai/Mixtral-8x7B-v0.1",
            "local_rank": null,
            "lora_alpha": 32,
            "model_type": "AutoModelForCausalLM",
            "num_epochs": 2,
            "wandb_name": "exp-6_lora-r-16_a-32-d-5e-2",
            "bench_split": "dharma_1_mini",
            "fsdp_config": null,
            "wandb_watch": true,
            "adam_epsilon": 0.00001,
            "load_in_4bit": true,
            "load_in_8bit": false,
            "lora_dropout": 0.05,
            "lr_scheduler": "cosine",
            "model_config": {
                "output_router_logits": true
            },
            "sequence_len": 16384,
            "val_set_size": 0.01,
            "wandb_entity": null,
            "warmup_steps": 25,
            "weight_decay": 0.1,
            "bench_dataset": "pharaouk/dharma-1/dharma_1_mini.json",
            "do_bench_eval": false,
            "learning_rate": 0.001,
            "logging_steps": 1,
            "max_grad_norm": 1,
            "wandb_project": "mixtral",
            "lora_model_dir": null,
            "sample_packing": true,
            "special_tokens": null,
            "tokenizer_type": "LlamaTokenizer",
            "flash_attention": true,
            "group_by_length": false,
            "train_on_inputs": false,
            "micro_batch_size": 1,
            "save_total_limit": 1,
            "trust_remote_code": true,
            "lora_target_linear": false,
            "xformers_attention": null,
            "axolotl_config_path": "mixtral_axolotl.yml",
            "lora_fan_in_fan_out": null,
            "lora_target_modules": [
                "q_proj",
                "k_proj",
                "v_proj",
                "o_proj"
            ],
            "pad_to_sequence_len": true,
            "unfrozen_parameters": null,
            "bench_source_max_len": 4096,
            "dataloader_pin_memory": true,
            "dataset_prepared_path": "last_run_prepared",
            "dataloader_num_workers": 8,
            "gradient_checkpointing": true,
            "resume_from_checkpoint": null,
            "early_stopping_patience": null,
            "dataloader_prefetch_factor": 4,
            "gradient_accumulation_steps": 4,
            "gradient_checkpointing_kwargs": {
                "use_reentrant": false
            }
        }
    },
    "entry_point": []
}